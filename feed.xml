<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://gustavofuhr.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gustavofuhr.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-06T17:45:41+00:00</updated><id>https://gustavofuhr.github.io/feed.xml</id><title type="html">Gustavo Führ</title><subtitle>PhD, Senior Machine Learning Engineer, Computer Vision Specialist.</subtitle><entry><title type="html">Deploying state-of-the-art object detectors (DETRs) to AWS.</title><link href="https://gustavofuhr.github.io/blog/2025/deploy-dfine-models/" rel="alternate" type="text/html" title="Deploying state-of-the-art object detectors (DETRs) to AWS."/><published>2025-01-01T11:34:00+00:00</published><updated>2025-01-01T11:34:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2025/deploy-dfine-models</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2025/deploy-dfine-models/"><![CDATA[<p>Forget about YOLO. Transformer-based models are better now and easy to deploy!</p> <p><strong>TL;DR</strong>: We use the awesome new transformer-based object detector D-FINE to create a custom image and use it for batch processing and model serving in AWS. We also performed inference tests for CPU/GPU and saw that you can achieve around 60% average precision on COCO in under 100ms per image. The code for deploying to AWS is in the <a href="https://gist.github.com/gustavofuhr/4a100858bfecc58845e35a495d3735bb">aws_utils.py</a> classes.</p> <h4 id="sota-of-object-detection">SOTA of object detection</h4> <p>Object detection is still a very popular task in computer vision, even with the introduction of large visual language models. That’s because, in most scenarios, we need to analyze the image quickly, on cheap hardware, and optimize for a particular set of objects via fine-tuning.In recent years, YOLO has been the most popular detector architecture for object detection, and it currently has 11 different versions from different people (I guess people like the name and use it to promote their detectors).</p> <p>Ultralytics has been the most prominent provider of YOLO models, providing an easy-to-use interface for different ML frameworks. However, the commercial license used is a big issue for most applications, so people are looking for Apache-licensed alternatives. As is often the case, the answer comes from the research community. Detection based on transformers is now outperforming YOLO models at the same parameter count (and inference times), a trend that began with <a href="https://arxiv.org/pdf/2304.08069">RT-DETR</a> [3]. The work that started with the original DETR paper [4] in 2020 has been perfected, and in the last few months, we got two additional models that are beating YOLO in performance: D-FINE [1] and DEIM [2]. Check out the graphs I took from the papers:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine-480.webp 480w,/assets/img/dfine-800.webp 800w,/assets/img/dfine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dfine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deim-480.webp 480w,/assets/img/deim-800.webp 800w,/assets/img/deim-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/deim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> From left to right: D-FINE [1] and DEIM [2] improve upon the SOTA and outperform all YOLO versions. </div> <h4 id="our-objective">Our objective</h4> <p>In this, <strong>we’re going to focus on D-FINE models</strong>, since they have a very nice interface, based on PyTorch. If you can, star the <a href="https://github.com/Peterande/D-FINE">project</a> on GitHub.The context where I deployed these models was a challenging surveillance scenario, aiming to detect people in low resolution, low lighting, and occasional occlusions. It’s always a good idea to test on your own dataset, since the common benchmark COCO might not translate well to your scenario. I did exactly that – check out the average precision in my data:</p> <div class="row mt-4"> <div class="col-sm mt-2 mt-md-0" style="padding-left: 15px; padding-right: 15px;"> <div class="d-none d-md-block" style="padding-left: 80px; padding-right: 80px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_performance-480.webp 480w,/assets/img/dfine_performance-800.webp 800w,/assets/img/dfine_performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dfine_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="d-block d-md-none"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_performance-480.webp 480w,/assets/img/dfine_performance-800.webp 800w,/assets/img/dfine_performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dfine_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption text-center"> Average precision in a challenging pedestrian dataset. D-FINE is very good! </div> <p>D-FINE performs amazingly, especially if we consider that all its versions are way smaller than, let’s say, the <a href="https://github.com/open-mmlab/mmdetection/tree/main/projects/CO-DETR">Co-DINO</a> model tested. In my case, I chose to deploy these D-FINE models trained on COCO, ignoring classes other than “person” for now.</p> <h4 id="generating-a-multi-purpose-docker-image">Generating a multi-purpose Docker image</h4> <p>Usually, the first step to using cloud ML services – and also a generally good idea – is to create your own Docker image for model inference. Here, there is a caveat: we want to use this model for both batch processing (with AWS Batch) and serving (with SageMaker). Using two different Docker images would be a pain, so we created a bash script (called <code class="language-plaintext highlighter-rouge">serve</code>) as an entry point to determine whether a script should run for a batch of images or if a Flask app should start to serve the model. Here’s the <code class="language-plaintext highlighter-rouge">Dockerfile</code>:</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> registry.cn-hangzhou.aliyuncs.com/peterande/dfine:v1</span>

<span class="k">WORKDIR</span><span class="s"> /workspace/</span>

<span class="k">RUN </span>git clone https://github.com/Peterande/D-FINE.git <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">cd </span>D-FINE <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">-r</span> tools/inference/requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install </span>opencv-python tqdm

<span class="k">RUN </span>pip <span class="nb">install </span>Flask opencv-python numpy requests Pillow flask-cors sagemaker-inference gunicorn 

<span class="k">COPY</span><span class="s"> dfine_checkpoints /workspace/dfine_checkpoints</span>
<span class="k">COPY</span><span class="s"> *.py /workspace/</span>
<span class="k">COPY</span><span class="s"> serve /usr/bin/serve</span>
<span class="k">RUN </span><span class="nb">chmod</span> +x /usr/bin/serve

<span class="k">ENV</span><span class="s"> PYTHONUNBUFFERED=1</span>
<span class="k">ENV</span><span class="s"> SAGEMAKER_PROGRAM=/workspace/app.py</span>

<span class="k">EXPOSE</span><span class="s"> 8080</span>

<span class="k">ENTRYPOINT</span><span class="s"> ["serve"]</span>
</code></pre></div></div> <p>Btw, <code class="language-plaintext highlighter-rouge">serve</code>, without <code class="language-plaintext highlighter-rouge">.sh</code> extension, is the expected name for a SageMaker custom model. The common parameters for the two running modes are the device (“cpu” or “cuda:x”), model configuration, and model checkpoint file. These parameters are kept in environment variables, making it easy to customize when deploying to SageMaker. The script looks like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nb">export </span><span class="nv">CONFIG_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">CONFIG_FILE</span><span class="k">:-</span><span class="s2">"/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml"</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">CHECKPOINT_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">CHECKPOINT_FILE</span><span class="k">:-</span><span class="s2">"/workspace/dfine_checkpoints/dfine_x_obj2coco.pth"</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">DEVICE</span><span class="o">=</span><span class="k">${</span><span class="nv">DEVICE</span><span class="k">:-</span><span class="s2">"cpu"</span><span class="k">}</span>  <span class="c"># Default to CPU if DEVICE is not set</span>

<span class="c"># Check if --batch is the first argument</span>
<span class="k">if</span> <span class="o">[[</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"--batch"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Running batch mode..."</span>
    <span class="c"># Shift to remove the first argument (--batch)</span>
    <span class="nb">shift
    </span>python <span class="nt">-u</span> /workspace/dfine_batch.py <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
<span class="k">else
    </span><span class="nb">echo</span> <span class="s2">"Running server mode..."</span>
    gunicorn <span class="nt">-w</span> 4 <span class="nt">-b</span> 0.0.0.0:8080 <span class="se">\</span>
        <span class="nt">--preload</span> <span class="se">\</span>
        <span class="nt">--timeout</span> 120 <span class="se">\</span>
        <span class="nt">--log-level</span> debug <span class="se">\</span>
        <span class="nt">--capture-output</span> <span class="se">\</span>
        <span class="nt">--timeout</span> 120 <span class="se">\</span>
        <span class="s2">"app:app"</span>
<span class="k">fi</span>
</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">dfine_batch.py</code> is a simple inference script that runs inference in a batch of images stored on an S3 bucket and <code class="language-plaintext highlighter-rouge">app.py</code> is a Flask app that interfaces with the model.</p> <p>Using the two files above, we can run the image with different parameters and the same entrypoint, so I would highly recommend you do the same. Of course, it’s possible to run this Docker image locally, with or without a GPU. For instance, here’s a sample command for batch processing images from S3 locally:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--rm</span> <span class="se">\</span>
    <span class="nt">-v</span> ~/.aws:/root/.aws <span class="se">\</span>
    <span class="nt">--gpus</span> all <span class="se">\</span>
    dfine_server <span class="se">\</span>
    <span class="nt">--batch</span> <span class="se">\</span>
    <span class="nt">--config_file</span> /workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml <span class="se">\</span>
    <span class="nt">--checkpoint_file</span> /workspace/dfine_checkpoints/dfine_x_obj2coco.pth <span class="se">\</span>
    <span class="nt">--input_s3_dir</span> s3://sample-combined/data/ <span class="se">\</span>
    <span class="nt">--output_s3</span> s3://sample-combined/output_results_computed_in_aws_batch.json
</code></pre></div></div> <p>You can also change the behavior of <code class="language-plaintext highlighter-rouge">dfine_batch.py</code> so that it gets images locally or does something else.</p> <h4 id="deploying-the-model-as-aws-batch-job">Deploying the model as AWS Batch Job</h4> <p>AWS Batch jobs are great because they provide reproducible ways of performing inference on different data without having to manage the infrastructure yourself. They usually work by creating an instance for the job, running the job, and then terminating the machines, which is nice and cheap. To submit a job, it’s necessary to create three configurations: the compute environment, the job queue and the job definition. It’s possible to do this in the web interface itself, via the REST API, or using the <code class="language-plaintext highlighter-rouge">boto3</code> Python library. I chose the latter and wrapped all the annoying stuff in the <code class="language-plaintext highlighter-rouge">AWSBatchJobClient</code> class in the <a href="https://gist.github.com/gustavofuhr/4a100858bfecc58845e35a495d3735bb"><code class="language-plaintext highlighter-rouge">aws_util.py</code></a> file. Using this helper class, creating a batch job boils down to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">COMPUTE_IN_GPU</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">batch_job_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dfine-inference-job</span><span class="sh">"</span>

<span class="n">instance_type</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c5.large</span><span class="sh">"</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">COMPUTE_IN_GPU</span> <span class="k">else</span> <span class="sh">"</span><span class="s">g4dn.xlarge</span><span class="sh">"</span>
<span class="n">subnets</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">subnet-something</span><span class="sh">"</span><span class="p">]</span>
<span class="n">security_group_ids</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">sg-0f0cd660a60052444</span><span class="sh">"</span><span class="p">]</span>
<span class="n">instance_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something/ecsTaskExecutionRole</span><span class="sh">"</span>
<span class="n">execution_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something/BatchJobExecutionRole</span><span class="sh">"</span>
<span class="n">container_props</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">container_repo/dfine_server</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">vcpus</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">memory</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">jobRoleArn</span><span class="sh">"</span><span class="p">:</span> <span class="n">execution_role_arn</span>
<span class="p">}</span>
<span class="n">containerOverrides</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">--batch</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--device</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">--config_file</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--checkpoint_file</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">/workspace/dfine_checkpoints/dfine_x_obj2coco.pth</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--input_s3_dir</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">s3://some-bucket/data/</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--output_s3</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">s3://some-bucket/output_results_computed_in_aws_batch.json</span><span class="sh">"</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="kn">from</span> <span class="n">aws_utils</span> <span class="kn">import</span> <span class="n">AWSBatchJobClient</span>
<span class="n">aws_batch_client</span> <span class="o">=</span> <span class="nc">AWSBatchJobClient</span><span class="p">(</span><span class="sh">"</span><span class="s">us-east-2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">delete_job_queue_if_exists</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-queue</span><span class="sh">"</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_compute_environment</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-env</span><span class="sh">"</span><span class="p">,</span> <span class="n">instance_role_arn</span><span class="p">,</span> <span class="n">instance_type</span><span class="p">,</span> <span class="n">subnets</span><span class="p">,</span> <span class="n">security_group_ids</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_job_queue</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-queue</span><span class="sh">"</span><span class="p">,</span> <span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-env</span><span class="sh">"</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_job_definition</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-job-def</span><span class="sh">"</span><span class="p">,</span> <span class="n">container_props</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>It may seem like a lot, but you’re mainly configuring how to create your instances (<code class="language-plaintext highlighter-rouge">instance_type</code>, <code class="language-plaintext highlighter-rouge">subnets</code>, <code class="language-plaintext highlighter-rouge">execution_role_arn</code>, etc.) and how to execute your Docker image in <code class="language-plaintext highlighter-rouge">containerOverrides</code>. Notice that using the <code class="language-plaintext highlighter-rouge">AWSBatchJobClient</code> methods with <code class="language-plaintext highlighter-rouge">overwrite=True</code> will delete configurations that exist in your AWS account, wait for them to be deactivated and properly deleted before continuing, so you can iterate quickly with your setup.</p> <h4 id="aws-sagemaker">AWS SageMaker</h4> <p>SageMaker offers powerful capabilities, including auto-scaling, model monitoring, and seamless integration with other AWS services. To serve a model in SageMaker, you need to implement an API with a few mandatory characteristics:</p> <ul> <li><code class="language-plaintext highlighter-rouge">invocations/</code> endpoint: the end-point that will be used for inference (in our case, oject detection).</li> <li><code class="language-plaintext highlighter-rouge">health/</code> endpoint: a route that returns 200 so that to check API status.</li> <li><code class="language-plaintext highlighter-rouge">EXPOSE 8080</code>: in Dockerfile, you should expose the 8080 port by default.</li> <li><code class="language-plaintext highlighter-rouge">serve</code> file: the already mentioned bash script.</li> </ul> <p>Given a container image with the above characteristics, you need to create some definitions, similar to AWS Batch: model, endpoint config, and finally the endpoint.To create the SageMaker model, you need to pass the Docker image’s URI (in ECR), the execution role ARN, and any environment variables you want to pass to the container upon running. The execution role attached to the model should have <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html?utm_source=chatgpt.com">specific permissions</a>, but you can use AmazonSageMakerFullAccess for debugging – if something is wrong, the model will be stuck at the “Creating” phase. The endpoint config should associate an instance type for the model to run and the endpoint itself only points to the config. Using the <code class="language-plaintext highlighter-rouge">SageMakerClient</code> from <code class="language-plaintext highlighter-rouge">aws_utils.py</code>, it would look like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dfine-x-obj2coco</span><span class="sh">"</span>
<span class="n">ecr_image_uri</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ecr_uri/dfine_server:latest</span><span class="sh">"</span>
<span class="n">execution_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something:role/service-role/AmazonSageMaker-ExecutionRole</span><span class="sh">"</span>

<span class="n">env_vars</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">CHECKPOINT_FILE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">/workspace/dfine_checkpoints/dfine_x_obj2coco.pth</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">CONFIG_FILE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">DEVICE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span>
<span class="p">}</span>
<span class="n">instance_type</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ml.g4dn.xlarge</span><span class="sh">"</span>

<span class="kn">from</span> <span class="n">aws_utils</span> <span class="kn">import</span> <span class="n">SageMakerClient</span>
<span class="n">c</span> <span class="o">=</span> <span class="nc">SageMakerClient</span><span class="p">(</span><span class="n">region_name</span><span class="o">=</span><span class="sh">"</span><span class="s">us-east-1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">c</span><span class="p">.</span><span class="nf">sagemaker_inference_deploy_pipeline</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">ecr_image_uri</span><span class="p">,</span> <span class="n">execution_role_arn</span><span class="p">,</span> <span class="n">env_vars</span><span class="p">,</span> <span class="n">instance_type</span><span class="p">)</span>    
</code></pre></div></div> <p>Easy, right? In the example above, I’m specifying that the endpoint should be launched in a GPU-enabled machine. For CPU, you need to change the instance type and the <code class="language-plaintext highlighter-rouge">DEVICE</code> environment variable.</p> <h4 id="inference-speed-profiling-local-vs-aws-sagemaker">Inference speed profiling (local vs. AWS SageMaker)</h4> <p>Inference times are important. There are too many GPU types, configurations, and model sizes to take into account, so I created some scripts to test the model’s performance. We tried three model sizes (<code class="language-plaintext highlighter-rouge">s</code>, <code class="language-plaintext highlighter-rouge">l</code> and <code class="language-plaintext highlighter-rouge">x</code>) running locally or on AWS, either GPU or CPU. To compute inference times and standard deviations, 100 requests were made to the API using random COCO val2017 images. Only the inference step was measured to exclude any other I/O latencies. The local machine has a GeForce RTX 2070 and a CPU i7-8700K and the AWS machines used were ml.m5.large for CPU and ml.g4dn.xlarge for GPU.</p> <p>Here’s the results:</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_speed_profiling-480.webp 480w,/assets/img/dfine_speed_profiling-800.webp 800w,/assets/img/dfine_speed_profiling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dfine_speed_profiling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Notice the change in scale between the two graphs. </div> <p>Looking at the CPU results, I would say that only the largest model (<code class="language-plaintext highlighter-rouge">dfine_x</code>) is not suitable for production in AWS. However, that may change if different CPU models are evaluated. For GPU performance, all methods are, on average, under 100ms for inference which is incredible – D-FINE-X model, which achieves 59.3% AP on COCO, included. Still, these times can be improved using other frameworks, such as TensorRT or ONNX Runtime.</p> <h4 id="thats-it">That’s it</h4> <p>I hope the snippets above help you. Contact me if you have any doubts about reproducing this post. I was surprised by how well D-FINE models behaved both in accuracy and speed, so that might be a game changer for me. I’m currently studying the many papers that stem from the original DETR work and might do a post on that.</p> <h4 id="references">References</h4> <p>[1] - Peng, Yansong, et al. “D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement.” arXiv preprint arXiv:2410.13842 (2024).</p> <p>[2] - Huang, Shihua, et al. “DEIM: DETR with Improved Matching for Fast Convergence.” arXiv preprint arXiv:2412.04234 (2024).</p> <p>[3] - Zhao, Yian, et al. “Detrs beat yolos on real-time object detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.</p> <p>[4] - Carion, Nicolas, et al. “End-to-end object detection with transformers.” European conference on computer vision. Cham: Springer International Publishing, 2020.</p>]]></content><author><name></name></author><category term="posts"/><category term="computer-vision"/><category term="dfine"/><category term="aws"/><category term="batch"/><category term="sagemaker"/><category term="deploy"/><category term="object"/><category term="detection"/><category term="yolo"/><summary type="html"><![CDATA[Forget about Yolo. Transformer-based models are better now, and easy to deploy!]]></summary></entry><entry><title type="html">ScreenshotQuery, make queries to screenshots using Vision Language Models.</title><link href="https://gustavofuhr.github.io/blog/2024/screenshot-query/" rel="alternate" type="text/html" title="ScreenshotQuery, make queries to screenshots using Vision Language Models."/><published>2024-12-04T11:34:00+00:00</published><updated>2024-12-04T11:34:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/screenshot-query</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/screenshot-query/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/screenshot_query_03.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> A notebook to query images. </div> <p>A couple of months ago I started a small project that I thought would be a simple task from my ever growing to-do list: processing approximately 1000 screenshots so I can perform some kind of image retrieval using natural language on them. It seemed simple enough yet it had some details that I would like to share in this post.</p> <p>Here’s the <a href="https://github.com/gustavofuhr/screenshot_query">project at Github</a>.</p> <p>🚨 Before we start: Screenshots are usually very personal and might contain a bunch of personal information. If you want to use this code, ensure that you are comfortable sending them to OpenAI or other LLMs.</p> <h4 id="which-problem-are-we-tackling-here">Which problem are we tackling here?</h4> <p>It’s important to clarify that we’ll delegate the image captioning to an MLLM model, even if there are other <a href="https://paperswithcode.com/task/image-captioning">available models</a> can run locally. If I started again I would probably try to use <a href="https://paperswithcode.com/paper/blip-2-bootstrapping-language-image-pre">BLIP-2</a> to generate captions/embeddings.</p> <p>So, our ultimate goal is to search images through textual content descriptions using natural language. Image retrieval (and scene classification) has a long history in Computer Vision – remember the seminal <a href="https://people.eecs.berkeley.edu/~efros/courses/AP06/Papers/csurka-eccv-04.pdf">Bag of Visual Words</a> model? <em>Yet, here we’re only trying to use natural language to query a set of image descriptions; everything is text</em>. The most appropriate nomenclature I found was “similarity search”, which is sometimes used in (now trending) RAG systems.</p> <p>Anyway, the next sections discuss the steps that I took to achieve the objective.</p> <h3 id="1-creating-image-descriptions">1. Creating image descriptions</h3> <p>The first thing is to send to a MLLM (Multi-modal Large Language Model), such as GPT4-o, all the images with a simple prompt asking the model to describe the image contents:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">get_image_description_openai</span><span class="p">(</span><span class="n">base64_image</span><span class="p">):</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Content-Type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">application/json</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Authorization</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Bearer </span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">OPENAI_API_KEY</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
    <span class="p">}</span>

    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What’s in this image?</span><span class="sh">"</span>
                <span class="p">},</span>
                <span class="p">{</span>
                <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">image_url</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">image_url</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="sh">"</span><span class="s">url</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">data:image/jpeg;base64,</span><span class="si">{</span><span class="n">base64_image</span><span class="si">}</span><span class="sh">"</span>
                <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">]</span>
            <span class="p">}</span>
        <span class="p">],</span>
        <span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">300</span>
    <span class="p">}</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">"</span><span class="s">https://api.openai.com/v1/chat/completions</span><span class="sh">"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">).</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="sh">"</span><span class="s">choices</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span>

</code></pre></div></div> <p>You can find the whole script <a href="https://github.com/gustavofuhr/screenshot_query/blob/main/generate_image_descriptions_openai.py">here</a>. That may take a lot of time, but it can be resumed multiple times. It’s also surprisingly cheap to send those images to OpenAI (for 1k images I guess it was around 2-3 USD total!). The script will create txt files describing the images, one for each image.</p> <h4 id="how-good-are-the-descriptions">How good are the descriptions?</h4> <p>It’s impressively good! Check these:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideShadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_02-480.webp 480w,/assets/img/screenshot_query/sq_desc_02-800.webp 800w,/assets/img/screenshot_query/sq_desc_02-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sq_desc_02.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_01-480.webp 480w,/assets/img/screenshot_query/sq_desc_01-800.webp 800w,/assets/img/screenshot_query/sq_desc_01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sq_desc_01.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_03-480.webp 480w,/assets/img/screenshot_query/sq_desc_03-800.webp 800w,/assets/img/screenshot_query/sq_desc_03-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sq_desc_03.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_04-480.webp 480w,/assets/img/screenshot_query/sq_desc_04-800.webp 800w,/assets/img/screenshot_query/sq_desc_04-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sq_desc_04.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h3 id="2-making-queries">2. Making queries</h3> <p>I got two main ideas for this:</p> <ul> <li>Create text embeddings for each description and perform an ANN (Approximate Nearest Neighbors) search for the query.</li> <li>Send the text containing all the image descriptions to the LLM with a prompt to retrieve the top-N most relevant image depictions.</li> </ul> <h4 id="emb">2.1 Text embeddings</h4> <p>The idea here is to represent the whole description of the image contents as a vector. That will also be done later for the query so we can search the closest embeddings to the query, under a certain distance threshold.I chose to use ANN since K-nearest neighbors will be too slow for larger datasets, in case I want to process my entire gallery someday.</p> <p>We’ll explore two ways to create these embeddings, but first, let’s talk about how we can store them and perform search. This is usually done with a vector database, and we can find several solutions for it: <a href="https://milvus.io/">Milvus</a>, <a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview">Vertex AI</a> from Google, <a href="https://github.com/weaviate/weaviate">Weaviate</a> etc. I had some experience in the first two but want to try Weaviate since it appeared to have a very easy setup. For search, it appears that Weaviate uses the <a href="https://arxiv.org/abs/1603.09320">HNSW algorithm</a> for ANN (Approximate Nearest Neighbors) which is probably available in the other solutions too – It’s worth mentioning that the awesome <a href="https://github.com/facebookresearch/faiss">Faiss</a> has implementations of several ANN algorithms if one wants to explore different methods.</p> <p>Using Weaviate was indeed very easy. Here’s how to start its Docker container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-p</span> 8080:8080 <span class="nt">-p</span> 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.26.3
</code></pre></div></div> <p>One thing that I found kind of annoying is that they recently changed the interface of the Python library. The version used in the following snippets is <code class="language-plaintext highlighter-rouge">weaviate-client==4.7.1</code>. First, you need to create a class, which is like a collection or a table if you’re not familiar with the term.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span> <span class="o">=</span> <span class="n">weaviate</span><span class="p">.</span><span class="nc">Client</span><span class="p">(</span><span class="n">WEAVIATE_URL</span><span class="p">)</span>

<span class="n">class_obj</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">class</span><span class="sh">"</span><span class="p">:</span> <span class="n">CLASS_NAME</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dataType</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">]},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dataType</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">]}</span>
    <span class="p">]</span>
<span class="p">}</span>
<span class="n">client</span><span class="p">.</span><span class="n">schema</span><span class="p">.</span><span class="nf">create_class</span><span class="p">(</span><span class="n">class_obj</span><span class="p">)</span>
</code></pre></div></div> <p>After that you can populate the class with the embeddings that you want:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span><span class="p">.</span><span class="n">data_object</span><span class="p">.</span><span class="nf">create</span><span class="p">({</span><span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">image.png</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">This is an image containing...}, 
                                CLASS_NAME, vector=description_embedding_vector)
</span></code></pre></div></div> <p>For searching you just need to provide three parameters: the query embedding, the min distance and the limit of objects to return:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="n">query</span><span class="p">.</span><span class="nf">near_vector</span><span class="p">(</span>
        <span class="n">near_vector</span><span class="o">=</span><span class="n">query_embedding</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">limit</span><span class="o">=</span><span class="n">n_images_limit</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>For creating the embeddings we explore two alternatives: SBERT and OpenAI API.</p> <h4 id="211-sbert-embeddings">2.1.1 SBERT embeddings</h4> <p>A couple of years ago, <a href="https://arxiv.org/abs/1810.04805">BERT</a> was the main option for creating text embeddings. It was used in a bunch of downstream tasks quite successfully. However, for comparing sentences, which is our goal, it’s not well suited mainly because it doesn’t generate fixed-size embeddings. If you want to use BERT for our case (semantic search), it would take a lot of time. From the <a href="https://arxiv.org/pdf/1908.10084">SBERT paper</a> abstract we have an impressive example:</p> <blockquote> <p>Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT.</p> </blockquote> <p>SBERT fixes this by focusing on semantic similarity tasks. With it, it’s possible to generate fixed embeddings for sentences (or paragraphs). For the above example, SBERT can reduce the elapsed time from ~65 hours to 5 seconds with the same accuracy.</p> <p>Here’s the code: (<a href="https://huggingface.co/sentence-transformers">Reference</a>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">feat</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div> <p>This will generate embeddings with a size of 384.</p> <h4 id="212-openai-embeddings">2.1.2 OpenAI embeddings</h4> <p>It’s expected that modern LLMs provide a way to get embeddings from text, and that is indeed the case. Since I wanted to use only the $10 USD credits I added to OpenAI for this project, I’m going to use their “text-embedding-3-small” model. Generating embeddings on the OpenAI API is super simple:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">text-embedding-3-small</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">text</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">"</span><span class="s">https://api.openai.com/v1/embeddings</span><span class="sh">"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">).</span><span class="nf">json</span><span class="p">()</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <p>Notice that there are <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models#embedding-models">larger models available</a> for computing embeddings that should give us better results. I just finished a small project on Text-to-SQL, and in that case, the use of larger models was indeed the deciding factor between success and failure (GPT4-o instead of GPT4-o-mini).</p> <h3 id="22-results-using-embeddings">2.2 Results using embeddings</h3> <p>Once the embeddings are created and stored in the Weaviate collection, we can query them, as we explained in the <a href="#emb">previous section</a>. To see if it is working I’m showing results for 4 different queries:</p> <ul> <li>Image showing any type of scale models.</li> <li>Screenshots showing smartphone lock screens.</li> <li>Screenshots that show research papers.</li> <li>Which screenshots show 3D printing models/objects or 3D printers.</li> </ul> <p>Here’s some results:</p> <p>SBERT embeddings:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideShadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_scale-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_scale-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_scale-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sbert/cropped_scale.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_screen-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_screen-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_screen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sbert/cropped_screen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_research-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_research-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_research-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sbert/cropped_research.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_3D-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_3D-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_3D-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/sbert/cropped_3D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p>It works, but there are some images with correct descriptions that should not be associated with the query. For instance, the first query has nothing to do with the screenshot of a research paper (4th screenshot in the first image), even though the description of the image is correct:</p> <blockquote> <p>The image seems to be a figure from a research paper titled “LOLNeRF: Learn from One Look.” It illustrates a method for reconstructing 3D models from 2D images using a neural network. Key components include:</p> <ul> <li><strong>Authors</strong>: The names of researchers associated with different institutions are listed.</li> <li><strong>Method Overview</strong>: It outlines a process involving coarse pose estimation and the use of a conditioned NeRF model for training and testing.</li> <li><strong>Images</strong>: Multiple images of faces appear to depict the results of the model, showing various views or representations of different individuals. The figure is likely meant to summarize the approach and findings of the research visually.</li> </ul> </blockquote> <p>The query about research papers also did not return very good examples. It has some weird matches that are not articles/papers at all.</p> <p>Open AI embeddings:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideShadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_scale-480.webp 480w,/assets/img/screenshot_query/openai/cropped_scale-800.webp 800w,/assets/img/screenshot_query/openai/cropped_scale-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/openai/cropped_scale.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_screen-480.webp 480w,/assets/img/screenshot_query/openai/cropped_screen-800.webp 800w,/assets/img/screenshot_query/openai/cropped_screen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/openai/cropped_screen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_research-480.webp 480w,/assets/img/screenshot_query/openai/cropped_research-800.webp 800w,/assets/img/screenshot_query/openai/cropped_research-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/openai/cropped_research.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_3D-480.webp 480w,/assets/img/screenshot_query/openai/cropped_3D-800.webp 800w,/assets/img/screenshot_query/openai/cropped_3D-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/screenshot_query/openai/cropped_3D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p>The results are similar but with fewer errors. That said, this entire analysis was qualitative. Conducting more structured experiments would be far more interesting (well, next time then).</p> <h3 id="23-asking-chatgpt-to-search">2.3 Asking chatGPT to search</h3> <p>LLMs perform incredibly well on zero-shot tasks, but sometimes it’s hard to have an intuition if they will solve your problem. I tried to send descriptions of all images with an instruction prompt like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_context</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Your job is to find the top {N_TOP_IMAGES} best screenshots based on a query and the image descriptions. 

Bellow there a bunch of image descriptors preceded by their filename. Make sure that, in your answer
you only include the image filenames in order from most relevant to least relevant from the chosen {N_TOP_IMAGES}.

Example, given a query like this:

</span><span class="sh">"</span><span class="s">Which screenshots appear to be tech products?</span><span class="sh">""</span><span class="s">

You should answer like, without text before this and without numbers before the image names (THIS IS AN FORMAT EXAMPLE, THESE IMAGES DON</span><span class="sh">'</span><span class="s">T EXISTS ):
IMAGE_01.PNG
IMAGE_02.jpeg
</span><span class="gp">...</span>

<span class="s">First there is a list of image filenames, only answer with these filenames in the order of relevance:

{image_filenames}

The following is a list of image descriptor (one for each image), you should use these to answer the query.

{image_descriptors}

</span><span class="sh">"""</span>
</code></pre></div></div> <p>Only using prompt engineering like that may work, but has two annoying problems. First, it is very easy to reach a token limit, even for less than a thousand images (one may be tempted to use Gemini or other APIs that offer longer context windows, but that won’t scale). Second, it is hard to force coherent outputs. For smaller models, it even returns image names that do not exist, and sometimes, the results come with a text preamble that changes each time and that needs to be removed for processing. In my research in text2SQL, that was mitigated using larger models (which don’t hallucinate as much) and by searching the part of the answer that actually corresponds to a formal SQL language.</p> <h2 id="3-thats-it">3. That’s it.</h2> <p>Using LLMs to help with side projects is awesome, especially if you are lazy. Things that would require two or three days of coding can be done in a few hours – with less quality for sure, but still a functional prototype that will give you an insight into whether your idea makes sense or not. That’s where this project comes from, the whole “chatGPT should work for that” new attitude that is changing programming for good. It’s still not as good as 90% of people that hype it up say, but it’s fun.</p> <p>I’m especially keen to ask the LLMs to structure the solution for the problem itself, so you can discover new ideas and tools that would be a pain to research by yourself.Asking how to solve the problem described here, chatGPT suggested using SBERT and Weaviate, which were outside my radar before that. Still, I think that the ideas here need a little more theoretical foundation so that we understand why sometimes completely unrelated texts may present near embeddings; maybe I will revisit this in the future.</p>]]></content><author><name></name></author><category term="posts"/><category term="computer-vision"/><category term="VLM"/><category term="image-retrieval"/><category term="image-caption"/><category term="semantic-search"/><summary type="html"><![CDATA[Describe and talk with your set of screenshots]]></summary></entry><entry><title type="html">Three random scripts.</title><link href="https://gustavofuhr.github.io/blog/2024/random-scripts/" rel="alternate" type="text/html" title="Three random scripts."/><published>2024-08-01T11:34:00+00:00</published><updated>2024-08-01T11:34:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/random-scripts</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/random-scripts/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mobilesam_annotator_sample-480.webp 480w,/assets/img/mobilesam_annotator_sample-800.webp 800w,/assets/img/mobilesam_annotator_sample-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mobilesam_annotator_sample.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MobileSAM single click annotation tool. </div> <p>In the last couple of months, I’ve been doing some random stuff and would like to share three random scripts. Hopefully, they will be useful for some people and indexed by search engines (if that’s a thing) or LLMs in the future. Here they are, in increasing order of randomness (from the perspective of a CV guy):</p> <h3 id="1-script-to-annotate-single-objects-in-a-bunch-of-images-mobilesam-annotator">1. Script to annotate single objects in a bunch of images, MobileSAM Annotator.</h3> <p>This script turned out into its own project, check the <a href="https://github.com/gustavofuhr/mobilesam_annotator">repo</a>.</p> <p>I’m trying to study some of the SAM (Segment Anything Model) papers in and noticed that in the <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf">original paper</a> they use the model to automatically annotate images which train the model in a training looping scheme. The foundation model itself is very powerful and could have a bunch of applications in the next few years in the industry and has recently (July 2024) been upgraded (<a href="https://ai.meta.com/blog/segment-anything-2/">SAM2</a>). I still believe that specific models will be more efficient for most problems, yet SAM models can be an easy starting point.</p> <p>For one application that I’m developing, I tried the <a href="https://segment-anything.com/demo">demo online</a> and found that it worked greatly and want it to run locally. I search for a few papers that attempted to speed-up these segmentation foundation models and find <a href="https://arxiv.org/pdf/2306.14289">MobileSAM</a>. I tried the <a href="https://github.com/ChaoningZhang/MobileSAM">provided models</a> and was surprised by how fast they are (running at around 220ms per image in my M3 Mac). Using it, I developed a single-point annotation tool, the GIF in the beginning of this post. The results are exported as separated binary images and visualizations are also stored so that you can better inspect any errors in the segmented regions. It worked great for my application and I still have some cool ideas to improve it: support for more points, more classes (objects) and integration with detection/segmentation frameworks.</p> <h3 id="2-script-to-rename-arxiv-files">2. Script to rename arXiv files:</h3> <p>Here’s the <a href="https://gist.github.com/gustavofuhr/83a7d5cb9bd93fdc6b74852a2f29dc67">script</a>.</p> <p>I read a lot of papers (or try to anyways) and the majority of them are available on the arXiv archive. Usually I download them to an iCloud-synced so that I can download/read papers on any device. But it always bothers me that the files have non-informative names with numbers, like <code class="language-plaintext highlighter-rouge">2403.05440.pdf</code>. I made a simple script to rename the files using the arXiv API. The API is free, and it was news for me that it even existed. It was quite easy to work with it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fetch_paper_details_arxiv</span><span class="p">(</span><span class="n">arxiv_number</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">http://export.arxiv.org/api/query?id_list=</span><span class="si">{</span><span class="n">arxiv_number</span><span class="si">}</span><span class="s">&amp;max_results=1</span><span class="sh">"</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">feedparser</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>
</code></pre></div></div> <p>tl;dr: it takes all the arXiv PDFs from a folder and rename them, such that</p> <p><code class="language-plaintext highlighter-rouge">2404.04125v3.pdf</code></p> <p>becomes</p> <p><code class="language-plaintext highlighter-rouge">2404.04125 - 2024 - No "Zero-Shot" Without Exponential Data, Pretraining Concept Frequency? Determines Multimodal Model Performance.pdf</code></p> <p>I know, it’s simple, but now you don’t have to do it :-).</p> <h3 id="3-script-to-get-notified-when-the-general-public-tickets-for-a-real-madrid-match-are-first-available">3. Script to get notified when the general public tickets for a Real Madrid match are first available.</h3> <p>This is a fun one (<a href="https://gist.github.com/gustavofuhr/4ce0edfc49d985f2094d4d4930a5761a">Gist link</a>).</p> <p>I’m a big fan of football: after all, I’m a Brazilian. During my last vacation, I fulfilled a long-awaited dream of watching live a Real Madrid match (if you’re curious it was against Alavés). If you don’t know, Real Madrid is the one the most famous teams in the world, so tickets are very hard to get. So, I made a script that checks periodically (every 5 min or so) if the tickets are available for sale; it will then send you an email. But how would I know if the script stopped running due to some problem? I also send a ping email every hour.</p> <p>For this script, I simply used Selenium to visit the webpage and check for a string that tell us tickets are available. I actually thought someone had already made this, but couldn’t find it. Anyways, the match was amazing, 5-0, two goals from Vini Jr.!</p> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/real_madrid-480.webp 480w,/assets/img/real_madrid-800.webp 800w,/assets/img/real_madrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/real_madrid.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bernabéu is gorgeous. </div>]]></content><author><name></name></author><category term="posts"/><category term="computer-vision"/><category term="arxiv"/><category term="real-madrid"/><summary type="html"><![CDATA[I made some cool little code snippets that I like it to share.]]></summary></entry><entry><title type="html">The future of OCR is not to use OCR!</title><link href="https://gustavofuhr.github.io/blog/2024/ocr-future/" rel="alternate" type="text/html" title="The future of OCR is not to use OCR!"/><published>2024-05-20T21:01:00+00:00</published><updated>2024-05-20T21:01:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/ocr-future</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/ocr-future/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ocr_future_img1-480.webp 480w,/assets/img/ocr_future_img1-800.webp 800w,/assets/img/ocr_future_img1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ocr_future_img1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image taken from [1], Donut paper. </div> <p>Around 3-4 years ago, some colleagues and I launched a product that cleverly used OCR outputs to retrieve information from document images. We usually explain that this product was intended for generic documents, meaning it was not trained for a specific layout or style. With a few simple rules (such as: give me the text below the label “Nome”) you can extract a structure JSON containing the most important information from these documents.</p> <p>The set of possible “rules” was pre-defined and mainly related to the OCRs positions and text without fully understanding them. Looking back, we failed to estimate how hard it was for users to create good rules for extracting the required information. Also, the pipeline for extracting info was very complex, resembling this:</p> <p>Get image -&gt; Detect text boxes -&gt; Read OCR -&gt; Aggregate OCR into blocks, lines -&gt; Apply rules for document -&gt; Output JSON.</p> <p>It was also difficult to adapt these pipelines to other important information that might be interesting to extract, such as checkmarks in forms and signatures.</p> <p>Some very cool methods for OCR-free document understanding appeared in the past few years. One of the first was Donut [1], by Clova (which also provided awesome OCRs and text detectors in the past). The idea was to go from the image to a JSON output with a single end-to-end model: Get image -&gt; Apply fine-tuned model for a given task -&gt; Output JSON. The presented results are impressive. The model is composed of an image encoder and text decoder built using transformers. The training is done (as it seems to be the norm these days) in two stages. First, in the pre-training phase, the task is to predict masked text in images (an objective analogous to OCR).</p> <p>Later the model is fine-tuned to a specific task, such as:</p> <ul> <li>document classification, i.e.: what kind of document it is?</li> <li>key information extraction, e.g.: extract the store address from this invoice</li> <li>VQA (Visual Question Answering), a generic question such as “Which is the document’s holder birthday?”</li> </ul> <p>This kind of solution is much more versatile and easy to implement in a product.</p> <p>To not extend this further, these are some other cool related works that I found: Dessurt [2]: similar to Donut, made around the same time, with less impressive results. Pix2Struct [3]: an extension of Donut for dealing with any type of image (documents, screenshots, UI). The pre-training was done with 80M screenshots of HTML images and the object was to predict the HTML of these images. How clever was that? ScreenAI [4]: a recent work from Google that tackles QA, UI navigation, etc. It uses LLMs to create huge annotated datasets. Can you imagine a Siri that actually can be useful and use apps?</p> <p>That’s it this week, let me know what you think.</p> <h2 id="references">References</h2> <p><em>[1] - Kim, Geewook, et al. “Ocr-free document understanding transformer.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.</em></p> <p><em>[2] - Davis, Brian, et al. “End-to-end document recognition and understanding with dessurt.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.</em></p> <p><em>[3] - Lee, Kenton, et al. “Pix2struct: Screenshot parsing as pretraining for visual language understanding.” International Conference on Machine Learning. PMLR, 2023.</em></p> <p><em>[4] - Baechler, Gilles, et al. “ScreenAI: A Vision-Language Model for UI and Infographics Understanding.” arXiv preprint arXiv:2402.04615 (2024).</em></p>]]></content><author><name></name></author><category term="posts"/><category term="ocr"/><category term="document-understanding"/><category term="computer-vision"/><summary type="html"><![CDATA[End-to-end document understanding will be huge.]]></summary></entry><entry><title type="html">Caution while using Large Language Models</title><link href="https://gustavofuhr.github.io/blog/2024/llm-failure/" rel="alternate" type="text/html" title="Caution while using Large Language Models"/><published>2024-02-11T21:01:00+00:00</published><updated>2024-02-11T21:01:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/llm-failure</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/llm-failure/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llm_failure_img1-480.webp 480w,/assets/img/llm_failure_img1-800.webp 800w,/assets/img/llm_failure_img1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/llm_failure_img1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image taken from [2]. </div> <p>In the past two years or so, everyone in the IA/ML is talking about Large Language Models (LLMs) and how to use them in a bunch of tasks. I’m also very interested in how these models can help us do better research and applications.</p> <p>We’re also seeing some clever applications of LLMs in Computer Vision beginning to appear. On example is the recent paper ScreenAI from Google [1] which uses LLMs (coupled with Vision models) to automatically generate a ton of useful training data.</p> <p>But today I want to highlight another cool paper by Alzahrani et al., in which they found how sensible to small changes in prompt are LLMs benchmarks and rankings. To attest this, in multiple choice question benchmark (MMLU), they change the options from [“A”, “B”, “C”, “D”] to something like [”$”, “&amp;”, “#”, “@”] and found that most LLMs perform worse merely due to this subtle change (it also completely change the ranking). Even worse, they notice that only swapping the position of answers could would also lead to a decrease in performance!</p> <p>The authors also refer to a nice work (still unpublished) by Den et al. 2023 [3] that tried to find if, for some LLMs, there was leakage for MMLU. In their experiment, they remove (wrong) answers from the question’s prompt and ask the LLM to predict the missing portion of the text. They found that most LLMs are able to recover these missing portions of the questions, which strongly suggest that somehow these benchmarks were used (perhaps not directly) in training.</p> <p>I think the takeaway here is that we need more consistent LLMs or methods (which are being researched) and that we should be careful in using these rankings/benchmarks to drive our choice of which model to use.</p> <h2 id="references">References</h2> <p><em>[1] - Baechler, Gilles, et al. “ScreenAI: A Vision-Language Model for UI and Infographics Understanding.” arXiv preprint arXiv:2402.04615 (2024).</em></p> <p><em>[2] - When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards: https://lnkd.in/dpBW3SAW</em></p> <p><em>[3] - Investigating Data Contamination in Modern Benchmarks for Large Language Models: https://lnkd.in/dBRdVVav</em></p>]]></content><author><name></name></author><category term="posts"/><category term="machine-learning"/><category term="large-language-models"/><category term="llm"/><summary type="html"><![CDATA[Weird failures and behavior from LLMs.]]></summary></entry><entry><title type="html">My favorite papers from CVPR 2022. | by Gustavo Führ | Medium</title><link href="https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/" rel="alternate" type="text/html" title="My favorite papers from CVPR 2022. | by Gustavo Führ | Medium"/><published>2022-08-08T00:00:00+00:00</published><updated>2022-08-08T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022--by-gustavo-fhr--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hello! Last June I attended (in person, yeah!) the CVPR 2022 held in New Orleans, with a bunch of cool people from work. I saw some amazing work, talked to a bunch of great people and I’m still…]]></summary></entry><entry><title type="html">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium"/><published>2017-05-18T00:00:00+00:00</published><updated>2017-05-18T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[At Meerkat, we improved our facial recognition by 40% at 10k distractors, with real-time performance and an easy interface. Facial recognition (FR) technology has come a long way in recent years in…]]></summary></entry><entry><title type="html">A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium"/><published>2017-04-19T00:00:00+00:00</published><updated>2017-04-19T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A new class of state-of-the-art object detectors from Computer Vision can provide novel insights in brand analysis. Social media analytics has greatly increased over the past decades, which showed to…]]></summary></entry><entry><title type="html">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium"/><published>2017-03-21T00:00:00+00:00</published><updated>2017-03-21T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[At Meerkat, we improved our facial recognition by 40% at 10k distractors, with real-time performance and an easy interface. Facial recognition (FR) technology has come a long way in recent years in…]]></summary></entry></feed>