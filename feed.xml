<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://gustavofuhr.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gustavofuhr.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-06T18:42:03+00:00</updated><id>https://gustavofuhr.github.io/feed.xml</id><title type="html">Gustavo Führ</title><subtitle>PhD, Senior Machine Learning Engineer, Computer Vision Specialist.</subtitle><entry><title type="html">Three random scripts.</title><link href="https://gustavofuhr.github.io/blog/2024/random-scripts/" rel="alternate" type="text/html" title="Three random scripts."/><published>2024-08-01T11:34:00+00:00</published><updated>2024-08-01T11:34:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/random-scripts</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/random-scripts/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mobilesam_annotator_sample-480.webp 480w,/assets/img/mobilesam_annotator_sample-800.webp 800w,/assets/img/mobilesam_annotator_sample-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mobilesam_annotator_sample.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MobileSAM single click annotation tool. </div> <p>In the last couple of months, I’ve been doing some random stuff and would like to share three random scripts. Hopefully, they will be useful for some people and indexed by search engines (if that’s a thing) or LLMs in the future. Here they are, in increasing order of randomness (from the perspective of a CV guy):</p> <h3 id="1-script-to-annotate-single-objects-in-a-bunch-of-images-mobilesam-annotator">1. Script to annotate single objects in a bunch of images, MobileSAM Annotator.</h3> <p>This script turned out into its own project, check the <a href="https://github.com/gustavofuhr/mobilesam_annotator">repo</a>.</p> <p>I’m trying to study some of the SAM (Segment Anything Model) papers in and noticed that in the <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf">original paper</a> they use the model to automatically annotate images which train the model in a training looping scheme. The foundation model itself is very powerful and could have a bunch of applications in the next few years in the industry and has recently (July 2024) been upgraded (<a href="https://ai.meta.com/blog/segment-anything-2/">SAM2</a>). I still believe that specific models will be more efficient for most problems, yet SAM models can be an easy starting point.</p> <p>For one application that I’m developing, I tried the <a href="https://segment-anything.com/demo">demo online</a> and found that it worked greatly and want it to run locally. I search for a few papers that attempted to speed-up these segmentation foundation models and find <a href="https://arxiv.org/pdf/2306.14289">MobileSAM</a>. I tried the <a href="https://github.com/ChaoningZhang/MobileSAM">provided models</a> and was surprised by how fast they are (running at around 220ms per image in my M3 Mac). Using it, I developed a single-point annotation tool, the GIF in the beginning of this post. The results are exported as separated binary images and visualizations are also stored so that you can better inspect any errors in the segmented regions. It worked great for my application and I still have some cool ideas to improve it: support for more points, more classes (objects) and integration with detection/segmentation frameworks.</p> <h3 id="2-script-to-rename-arxiv-files">2. Script to rename arXiv files:</h3> <p>Here’s the <a href="https://gist.github.com/gustavofuhr/83a7d5cb9bd93fdc6b74852a2f29dc67">script</a>.</p> <p>I read a lot of papers (or try to anyways) and the majority of them are available on the arXiv archive. Usually I download them to an iCloud-synced so that I can download/read papers on any device. But it always bothers me that the files have non-informative names with numbers, like <code class="language-plaintext highlighter-rouge">2403.05440.pdf</code>. I made a simple script to rename the files using the arXiv API. The API is free, and it was news for me that it even existed. It was quite easy to work with it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fetch_paper_details_arxiv</span><span class="p">(</span><span class="n">arxiv_number</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">http://export.arxiv.org/api/query?id_list=</span><span class="si">{</span><span class="n">arxiv_number</span><span class="si">}</span><span class="s">&amp;max_results=1</span><span class="sh">"</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">feedparser</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>
</code></pre></div></div> <p>tl;dr: it takes all the arXiv PDFs from a folder and rename them, such that</p> <p><code class="language-plaintext highlighter-rouge">2404.04125v3.pdf</code></p> <p>becomes</p> <p><code class="language-plaintext highlighter-rouge">2404.04125 - 2024 - No "Zero-Shot" Without Exponential Data, Pretraining Concept Frequency? Determines Multimodal Model Performance.pdf</code></p> <p>I know, it’s simple, but now you don’t have to do it :-).</p> <h3 id="3-script-to-get-notified-when-the-general-public-tickets-for-a-real-madrid-match-are-first-available">3. Script to get notified when the general public tickets for a Real Madrid match are first available.</h3> <p>This is a fun one (<a href="https://gist.github.com/gustavofuhr/4ce0edfc49d985f2094d4d4930a5761a">Gist link</a>).</p> <p>I’m a big fan of football: after all, I’m a Brazilian. During my last vacation, I fulfilled a long-awaited dream of watching live a Real Madrid match (if you’re curious it was against Alavés). If you don’t know, Real Madrid is the one the most famous teams in the world, so tickets are very hard to get. So, I made a script that checks periodically (every 5 min or so) if the tickets are available for sale; it will then send you an email. But how would I know if the script stopped running due to some problem? I also send a ping email every hour.</p> <p>For this script, I simply used Selenium to visit the webpage and check for a string that tell us tickets are available. I actually thought someone had already made this, but couldn’t find it. Anyways, the match was amazing, 5-0, two goals from Vini Jr.!</p> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/real_madrid-480.webp 480w,/assets/img/real_madrid-800.webp 800w,/assets/img/real_madrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/real_madrid.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bernabéu is gorgeous. </div>]]></content><author><name></name></author><category term="posts"/><category term="computer-vision"/><category term="arxiv"/><category term="real-madrid"/><summary type="html"><![CDATA[I made some cool little code snippets that I like it to share.]]></summary></entry><entry><title type="html">The future of OCR is not to use OCR!</title><link href="https://gustavofuhr.github.io/blog/2024/ocr-future/" rel="alternate" type="text/html" title="The future of OCR is not to use OCR!"/><published>2024-05-20T21:01:00+00:00</published><updated>2024-05-20T21:01:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/ocr-future</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/ocr-future/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ocr_future_img1-480.webp 480w,/assets/img/ocr_future_img1-800.webp 800w,/assets/img/ocr_future_img1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ocr_future_img1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image taken from [1], Donut paper. </div> <p>Around 3-4 years ago, some colleagues and I launched a product that cleverly used OCR outputs to retrieve information from document images. We usually explain that this product was intended for generic documents, meaning it was not trained for a specific layout or style. With a few simple rules (such as: give me the text below the label “Nome”) you can extract a structure JSON containing the most important information from these documents.</p> <p>The set of possible “rules” was pre-defined and mainly related to the OCRs positions and text without fully understanding them. Looking back, we failed to estimate how hard it was for users to create good rules for extracting the required information. Also, the pipeline for extracting info was very complex, resembling this:</p> <p>Get image -&gt; Detect text boxes -&gt; Read OCR -&gt; Aggregate OCR into blocks, lines -&gt; Apply rules for document -&gt; Output JSON.</p> <p>It was also difficult to adapt these pipelines to other important information that might be interesting to extract, such as checkmarks in forms and signatures.</p> <p>Some very cool methods for OCR-free document understanding appeared in the past few years. One of the first was Donut [1], by Clova (which also provided awesome OCRs and text detectors in the past). The idea was to go from the image to a JSON output with a single end-to-end model: Get image -&gt; Apply fine-tuned model for a given task -&gt; Output JSON. The presented results are impressive. The model is composed of an image encoder and text decoder built using transformers. The training is done (as it seems to be the norm these days) in two stages. First, in the pre-training phase, the task is to predict masked text in images (an objective analogous to OCR).</p> <p>Later the model is fine-tuned to a specific task, such as:</p> <ul> <li>document classification, i.e.: what kind of document it is?</li> <li>key information extraction, e.g.: extract the store address from this invoice</li> <li>VQA (Visual Question Answering), a generic question such as “Which is the document’s holder birthday?”</li> </ul> <p>This kind of solution is much more versatile and easy to implement in a product.</p> <p>To not extend this further, these are some other cool related works that I found: Dessurt [2]: similar to Donut, made around the same time, with less impressive results. Pix2Struct [3]: an extension of Donut for dealing with any type of image (documents, screenshots, UI). The pre-training was done with 80M screenshots of HTML images and the object was to predict the HTML of these images. How clever was that? ScreenAI [4]: a recent work from Google that tackles QA, UI navigation, etc. It uses LLMs to create huge annotated datasets. Can you imagine a Siri that actually can be useful and use apps?</p> <p>That’s it this week, let me know what you think.</p> <h2 id="references">References</h2> <p><em>[1] - Kim, Geewook, et al. “Ocr-free document understanding transformer.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.</em></p> <p><em>[2] - Davis, Brian, et al. “End-to-end document recognition and understanding with dessurt.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.</em></p> <p><em>[3] - Lee, Kenton, et al. “Pix2struct: Screenshot parsing as pretraining for visual language understanding.” International Conference on Machine Learning. PMLR, 2023.</em></p> <p><em>[4] - Baechler, Gilles, et al. “ScreenAI: A Vision-Language Model for UI and Infographics Understanding.” arXiv preprint arXiv:2402.04615 (2024).</em></p>]]></content><author><name></name></author><category term="posts"/><category term="ocr"/><category term="document-understanding"/><category term="computer-vision"/><summary type="html"><![CDATA[End-to-end document understanding will be huge.]]></summary></entry><entry><title type="html">Caution while using Large Language Models</title><link href="https://gustavofuhr.github.io/blog/2024/llm-failure/" rel="alternate" type="text/html" title="Caution while using Large Language Models"/><published>2024-02-11T21:01:00+00:00</published><updated>2024-02-11T21:01:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2024/llm-failure</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2024/llm-failure/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llm_failure_img1-480.webp 480w,/assets/img/llm_failure_img1-800.webp 800w,/assets/img/llm_failure_img1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/llm_failure_img1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image taken from [2]. </div> <p>In the past two years or so, everyone in the IA/ML is talking about Large Language Models (LLMs) and how to use them in a bunch of tasks. I’m also very interested in how these models can help us do better research and applications.</p> <p>We’re also seeing some clever applications of LLMs in Computer Vision beginning to appear. On example is the recent paper ScreenAI from Google [1] which uses LLMs (coupled with Vision models) to automatically generate a ton of useful training data.</p> <p>But today I want to highlight another cool paper by Alzahrani et al., in which they found how sensible to small changes in prompt are LLMs benchmarks and rankings. To attest this, in multiple choice question benchmark (MMLU), they change the options from [“A”, “B”, “C”, “D”] to something like [”$”, “&amp;”, “#”, “@”] and found that most LLMs perform worse merely due to this subtle change (it also completely change the ranking). Even worse, they notice that only swapping the position of answers could would also lead to a decrease in performance!</p> <p>The authors also refer to a nice work (still unpublished) by Den et al. 2023 [3] that tried to find if, for some LLMs, there was leakage for MMLU. In their experiment, they remove (wrong) answers from the question’s prompt and ask the LLM to predict the missing portion of the text. They found that most LLMs are able to recover these missing portions of the questions, which strongly suggest that somehow these benchmarks were used (perhaps not directly) in training.</p> <p>I think the takeaway here is that we need more consistent LLMs or methods (which are being researched) and that we should be careful in using these rankings/benchmarks to drive our choice of which model to use.</p> <h2 id="references">References</h2> <p><em>[1] - Baechler, Gilles, et al. “ScreenAI: A Vision-Language Model for UI and Infographics Understanding.” arXiv preprint arXiv:2402.04615 (2024).</em></p> <p><em>[2] - When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards: https://lnkd.in/dpBW3SAW</em></p> <p><em>[3] - Investigating Data Contamination in Modern Benchmarks for Large Language Models: https://lnkd.in/dBRdVVav</em></p>]]></content><author><name></name></author><category term="posts"/><category term="machine-learning"/><category term="large-language-models"/><category term="llm"/><summary type="html"><![CDATA[Weird failures and behavior from LLMs.]]></summary></entry><entry><title type="html">My favorite papers from CVPR 2022. | by Gustavo Führ | Medium</title><link href="https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/" rel="alternate" type="text/html" title="My favorite papers from CVPR 2022. | by Gustavo Führ | Medium"/><published>2022-08-08T00:00:00+00:00</published><updated>2022-08-08T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022--by-gustavo-fhr--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hello! Last June I attended (in person, yeah!) the CVPR 2022 held in New Orleans, with a bunch of cool people from work. I saw some amazing work, talked to a bunch of great people and I’m still…]]></summary></entry><entry><title type="html">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium"/><published>2017-05-18T00:00:00+00:00</published><updated>2017-05-18T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[At Meerkat, we improved our facial recognition by 40% at 10k distractors, with real-time performance and an easy interface. Facial recognition (FR) technology has come a long way in recent years in…]]></summary></entry><entry><title type="html">A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium"/><published>2017-04-19T00:00:00+00:00</published><updated>2017-04-19T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A new class of state-of-the-art object detectors from Computer Vision can provide novel insights in brand analysis. Social media analytics has greatly increased over the past decades, which showed to…]]></summary></entry><entry><title type="html">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</title><link href="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/" rel="alternate" type="text/html" title="Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium"/><published>2017-03-21T00:00:00+00:00</published><updated>2017-03-21T00:00:00+00:00</updated><id>https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems--by-meerkat-cv--medium</id><content type="html" xml:base="https://gustavofuhr.github.io/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[At Meerkat, we improved our facial recognition by 40% at 10k distractors, with real-time performance and an easy interface. Facial recognition (FR) technology has come a long way in recent years in…]]></summary></entry></feed>