<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deploying state-of-the-art object detectors (DETRs) to AWS. | Gustavo Führ </title> <meta name="author" content="Gustavo Führ"> <meta name="description" content="Forget about Yolo. Transformer-based models are better now, and easy to deploy!"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?491bdee39dab28e6dfd8bfd8a4b8ac78"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gustavofuhr.github.io/blog/2025/deploy-dfine-models/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Gustavo Führ </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repos </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deploying state-of-the-art object detectors (DETRs) to AWS.</h1> <p class="post-meta"> Created in January 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   <a href="/blog/tag/dfine"> <i class="fa-solid fa-hashtag fa-sm"></i> dfine</a>   <a href="/blog/tag/aws"> <i class="fa-solid fa-hashtag fa-sm"></i> aws</a>   <a href="/blog/tag/batch"> <i class="fa-solid fa-hashtag fa-sm"></i> batch</a>   <a href="/blog/tag/sagemaker"> <i class="fa-solid fa-hashtag fa-sm"></i> sagemaker</a>   <a href="/blog/tag/deploy"> <i class="fa-solid fa-hashtag fa-sm"></i> deploy</a>   <a href="/blog/tag/object"> <i class="fa-solid fa-hashtag fa-sm"></i> object</a>   <a href="/blog/tag/detection"> <i class="fa-solid fa-hashtag fa-sm"></i> detection</a>   <a href="/blog/tag/yolo"> <i class="fa-solid fa-hashtag fa-sm"></i> yolo</a>   ·   <a href="/blog/category/posts"> <i class="fa-solid fa-tag fa-sm"></i> posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Forget about YOLO. Transformer-based models are better now and easy to deploy!</p> <p><strong>TL;DR</strong>: We use the awesome new transformer-based object detector D-FINE to create a custom image and use it for batch processing and model serving in AWS. We also performed inference tests for CPU/GPU and saw that you can achieve around 60% average precision on COCO in under 100ms per image. The code for deploying to AWS is in the <a href="https://gist.github.com/gustavofuhr/4a100858bfecc58845e35a495d3735bb" rel="external nofollow noopener" target="_blank">aws_utils.py</a> classes.</p> <h4 id="sota-of-object-detection">SOTA of object detection</h4> <p>Object detection is still a very popular task in computer vision, even with the introduction of large visual language models. That’s because, in most scenarios, we need to analyze the image quickly, on cheap hardware, and optimize for a particular set of objects via fine-tuning.In recent years, YOLO has been the most popular detector architecture for object detection, and it currently has 11 different versions from different people (I guess people like the name and use it to promote their detectors).</p> <p>Ultralytics has been the most prominent provider of YOLO models, providing an easy-to-use interface for different ML frameworks. However, the commercial license used is a big issue for most applications, so people are looking for Apache-licensed alternatives. As is often the case, the answer comes from the research community. Detection based on transformers is now outperforming YOLO models at the same parameter count (and inference times), a trend that began with <a href="https://arxiv.org/pdf/2304.08069" rel="external nofollow noopener" target="_blank">RT-DETR</a> [3]. The work that started with the original DETR paper [4] in 2020 has been perfected, and in the last few months, we got two additional models that are beating YOLO in performance: D-FINE [1] and DEIM [2]. Check out the graphs I took from the papers:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine-480.webp 480w,/assets/img/dfine-800.webp 800w,/assets/img/dfine-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dfine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deim-480.webp 480w,/assets/img/deim-800.webp 800w,/assets/img/deim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/deim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> From left to right: D-FINE [1] and DEIM [2] improve upon the SOTA and outperform all YOLO versions. </div> <h4 id="our-objective">Our objective</h4> <p>In this, <strong>we’re going to focus on D-FINE models</strong>, since they have a very nice interface, based on PyTorch. If you can, star the <a href="https://github.com/Peterande/D-FINE" rel="external nofollow noopener" target="_blank">project</a> on GitHub.The context where I deployed these models was a challenging surveillance scenario, aiming to detect people in low resolution, low lighting, and occasional occlusions. It’s always a good idea to test on your own dataset, since the common benchmark COCO might not translate well to your scenario. I did exactly that – check out the average precision in my data:</p> <div class="row mt-4"> <div class="col-sm mt-2 mt-md-0" style="padding-left: 15px; padding-right: 15px;"> <div class="d-none d-md-block" style="padding-left: 80px; padding-right: 80px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_performance-480.webp 480w,/assets/img/dfine_performance-800.webp 800w,/assets/img/dfine_performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dfine_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="d-block d-md-none"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_performance-480.webp 480w,/assets/img/dfine_performance-800.webp 800w,/assets/img/dfine_performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dfine_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption text-center"> Average precision in a challenging pedestrian dataset. D-FINE is very good! </div> <p>D-FINE performs amazingly, especially if we consider that all its versions are way smaller than, let’s say, the <a href="https://github.com/open-mmlab/mmdetection/tree/main/projects/CO-DETR" rel="external nofollow noopener" target="_blank">Co-DINO</a> model tested. In my case, I chose to deploy these D-FINE models trained on COCO, ignoring classes other than “person” for now.</p> <h4 id="generating-a-multi-purpose-docker-image">Generating a multi-purpose Docker image</h4> <p>Usually, the first step to using cloud ML services – and also a generally good idea – is to create your own Docker image for model inference. Here, there is a caveat: we want to use this model for both batch processing (with AWS Batch) and serving (with SageMaker). Using two different Docker images would be a pain, so we created a bash script (called <code class="language-plaintext highlighter-rouge">serve</code>) as an entry point to determine whether a script should run for a batch of images or if a Flask app should start to serve the model. Here’s the <code class="language-plaintext highlighter-rouge">Dockerfile</code>:</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> registry.cn-hangzhou.aliyuncs.com/peterande/dfine:v1</span>

<span class="k">WORKDIR</span><span class="s"> /workspace/</span>

<span class="k">RUN </span>git clone https://github.com/Peterande/D-FINE.git <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">cd </span>D-FINE <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">-r</span> tools/inference/requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install </span>opencv-python tqdm

<span class="k">RUN </span>pip <span class="nb">install </span>Flask opencv-python numpy requests Pillow flask-cors sagemaker-inference gunicorn 

<span class="k">COPY</span><span class="s"> dfine_checkpoints /workspace/dfine_checkpoints</span>
<span class="k">COPY</span><span class="s"> *.py /workspace/</span>
<span class="k">COPY</span><span class="s"> serve /usr/bin/serve</span>
<span class="k">RUN </span><span class="nb">chmod</span> +x /usr/bin/serve

<span class="k">ENV</span><span class="s"> PYTHONUNBUFFERED=1</span>
<span class="k">ENV</span><span class="s"> SAGEMAKER_PROGRAM=/workspace/app.py</span>

<span class="k">EXPOSE</span><span class="s"> 8080</span>

<span class="k">ENTRYPOINT</span><span class="s"> ["serve"]</span>
</code></pre></div></div> <p>Btw, <code class="language-plaintext highlighter-rouge">serve</code>, without <code class="language-plaintext highlighter-rouge">.sh</code> extension, is the expected name for a SageMaker custom model. The common parameters for the two running modes are the device (“cpu” or “cuda:x”), model configuration, and model checkpoint file. These parameters are kept in environment variables, making it easy to customize when deploying to SageMaker. The script looks like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nb">export </span><span class="nv">CONFIG_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">CONFIG_FILE</span><span class="k">:-</span><span class="s2">"/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml"</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">CHECKPOINT_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">CHECKPOINT_FILE</span><span class="k">:-</span><span class="s2">"/workspace/dfine_checkpoints/dfine_x_obj2coco.pth"</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">DEVICE</span><span class="o">=</span><span class="k">${</span><span class="nv">DEVICE</span><span class="k">:-</span><span class="s2">"cpu"</span><span class="k">}</span>  <span class="c"># Default to CPU if DEVICE is not set</span>

<span class="c"># Check if --batch is the first argument</span>
<span class="k">if</span> <span class="o">[[</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"--batch"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Running batch mode..."</span>
    <span class="c"># Shift to remove the first argument (--batch)</span>
    <span class="nb">shift
    </span>python <span class="nt">-u</span> /workspace/dfine_batch.py <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
<span class="k">else
    </span><span class="nb">echo</span> <span class="s2">"Running server mode..."</span>
    gunicorn <span class="nt">-w</span> 4 <span class="nt">-b</span> 0.0.0.0:8080 <span class="se">\</span>
        <span class="nt">--preload</span> <span class="se">\</span>
        <span class="nt">--timeout</span> 120 <span class="se">\</span>
        <span class="nt">--log-level</span> debug <span class="se">\</span>
        <span class="nt">--capture-output</span> <span class="se">\</span>
        <span class="nt">--timeout</span> 120 <span class="se">\</span>
        <span class="s2">"app:app"</span>
<span class="k">fi</span>
</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">dfine_batch.py</code> is a simple inference script that runs inference in a batch of images stored on an S3 bucket and <code class="language-plaintext highlighter-rouge">app.py</code> is a Flask app that interfaces with the model.</p> <p>Using the two files above, we can run the image with different parameters and the same entrypoint, so I would highly recommend you do the same. Of course, it’s possible to run this Docker image locally, with or without a GPU. For instance, here’s a sample command for batch processing images from S3 locally:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--rm</span> <span class="se">\</span>
    <span class="nt">-v</span> ~/.aws:/root/.aws <span class="se">\</span>
    <span class="nt">--gpus</span> all <span class="se">\</span>
    dfine_server <span class="se">\</span>
    <span class="nt">--batch</span> <span class="se">\</span>
    <span class="nt">--config_file</span> /workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml <span class="se">\</span>
    <span class="nt">--checkpoint_file</span> /workspace/dfine_checkpoints/dfine_x_obj2coco.pth <span class="se">\</span>
    <span class="nt">--input_s3_dir</span> s3://sample-combined/data/ <span class="se">\</span>
    <span class="nt">--output_s3</span> s3://sample-combined/output_results_computed_in_aws_batch.json
</code></pre></div></div> <p>You can also change the behavior of <code class="language-plaintext highlighter-rouge">dfine_batch.py</code> so that it gets images locally or does something else.</p> <h4 id="deploying-the-model-as-aws-batch-job">Deploying the model as AWS Batch Job</h4> <p>AWS Batch jobs are great because they provide reproducible ways of performing inference on different data without having to manage the infrastructure yourself. They usually work by creating an instance for the job, running the job, and then terminating the machines, which is nice and cheap. To submit a job, it’s necessary to create three configurations: the compute environment, the job queue and the job definition. It’s possible to do this in the web interface itself, via the REST API, or using the <code class="language-plaintext highlighter-rouge">boto3</code> Python library. I chose the latter and wrapped all the annoying stuff in the <code class="language-plaintext highlighter-rouge">AWSBatchJobClient</code> class in the <a href="https://gist.github.com/gustavofuhr/4a100858bfecc58845e35a495d3735bb" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">aws_util.py</code></a> file. Using this helper class, creating a batch job boils down to:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">COMPUTE_IN_GPU</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">batch_job_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dfine-inference-job</span><span class="sh">"</span>

<span class="n">instance_type</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c5.large</span><span class="sh">"</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">COMPUTE_IN_GPU</span> <span class="k">else</span> <span class="sh">"</span><span class="s">g4dn.xlarge</span><span class="sh">"</span>
<span class="n">subnets</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">subnet-something</span><span class="sh">"</span><span class="p">]</span>
<span class="n">security_group_ids</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">sg-0f0cd660a60052444</span><span class="sh">"</span><span class="p">]</span>
<span class="n">instance_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something/ecsTaskExecutionRole</span><span class="sh">"</span>
<span class="n">execution_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something/BatchJobExecutionRole</span><span class="sh">"</span>
<span class="n">container_props</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">container_repo/dfine_server</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">vcpus</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">memory</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">jobRoleArn</span><span class="sh">"</span><span class="p">:</span> <span class="n">execution_role_arn</span>
<span class="p">}</span>
<span class="n">containerOverrides</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">command</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">--batch</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--device</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">--config_file</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--checkpoint_file</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">/workspace/dfine_checkpoints/dfine_x_obj2coco.pth</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--input_s3_dir</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">s3://some-bucket/data/</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">--output_s3</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">s3://some-bucket/output_results_computed_in_aws_batch.json</span><span class="sh">"</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="kn">from</span> <span class="n">aws_utils</span> <span class="kn">import</span> <span class="n">AWSBatchJobClient</span>
<span class="n">aws_batch_client</span> <span class="o">=</span> <span class="nc">AWSBatchJobClient</span><span class="p">(</span><span class="sh">"</span><span class="s">us-east-2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">delete_job_queue_if_exists</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-queue</span><span class="sh">"</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_compute_environment</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-env</span><span class="sh">"</span><span class="p">,</span> <span class="n">instance_role_arn</span><span class="p">,</span> <span class="n">instance_type</span><span class="p">,</span> <span class="n">subnets</span><span class="p">,</span> <span class="n">security_group_ids</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_job_queue</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-queue</span><span class="sh">"</span><span class="p">,</span> <span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-env</span><span class="sh">"</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">aws_batch_client</span><span class="p">.</span><span class="nf">create_job_definition</span><span class="p">(</span><span class="n">batch_job_name</span><span class="o">+</span><span class="sh">"</span><span class="s">-job-def</span><span class="sh">"</span><span class="p">,</span> <span class="n">container_props</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>It may seem like a lot, but you’re mainly configuring how to create your instances (<code class="language-plaintext highlighter-rouge">instance_type</code>, <code class="language-plaintext highlighter-rouge">subnets</code>, <code class="language-plaintext highlighter-rouge">execution_role_arn</code>, etc.) and how to execute your Docker image in <code class="language-plaintext highlighter-rouge">containerOverrides</code>. Notice that using the <code class="language-plaintext highlighter-rouge">AWSBatchJobClient</code> methods with <code class="language-plaintext highlighter-rouge">overwrite=True</code> will delete configurations that exist in your AWS account, wait for them to be deactivated and properly deleted before continuing, so you can iterate quickly with your setup.</p> <h4 id="aws-sagemaker">AWS SageMaker</h4> <p>SageMaker offers powerful capabilities, including auto-scaling, model monitoring, and seamless integration with other AWS services. To serve a model in SageMaker, you need to implement an API with a few mandatory characteristics:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">invocations/</code> endpoint: the end-point that will be used for inference (in our case, oject detection).</li> <li> <code class="language-plaintext highlighter-rouge">health/</code> endpoint: a route that returns 200 so that to check API status.</li> <li> <code class="language-plaintext highlighter-rouge">EXPOSE 8080</code>: in Dockerfile, you should expose the 8080 port by default.</li> <li> <code class="language-plaintext highlighter-rouge">serve</code> file: the already mentioned bash script.</li> </ul> <p>Given a container image with the above characteristics, you need to create some definitions, similar to AWS Batch: model, endpoint config, and finally the endpoint.To create the SageMaker model, you need to pass the Docker image’s URI (in ECR), the execution role ARN, and any environment variables you want to pass to the container upon running. The execution role attached to the model should have <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">specific permissions</a>, but you can use AmazonSageMakerFullAccess for debugging – if something is wrong, the model will be stuck at the “Creating” phase. The endpoint config should associate an instance type for the model to run and the endpoint itself only points to the config. Using the <code class="language-plaintext highlighter-rouge">SageMakerClient</code> from <code class="language-plaintext highlighter-rouge">aws_utils.py</code>, it would look like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dfine-x-obj2coco</span><span class="sh">"</span>
<span class="n">ecr_image_uri</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ecr_uri/dfine_server:latest</span><span class="sh">"</span>
<span class="n">execution_role_arn</span> <span class="o">=</span> <span class="sh">"</span><span class="s">arn:aws:iam::something:role/service-role/AmazonSageMaker-ExecutionRole</span><span class="sh">"</span>

<span class="n">env_vars</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">CHECKPOINT_FILE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">/workspace/dfine_checkpoints/dfine_x_obj2coco.pth</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">CONFIG_FILE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">/workspace/D-FINE/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">DEVICE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span>
<span class="p">}</span>
<span class="n">instance_type</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ml.g4dn.xlarge</span><span class="sh">"</span>

<span class="kn">from</span> <span class="n">aws_utils</span> <span class="kn">import</span> <span class="n">SageMakerClient</span>
<span class="n">c</span> <span class="o">=</span> <span class="nc">SageMakerClient</span><span class="p">(</span><span class="n">region_name</span><span class="o">=</span><span class="sh">"</span><span class="s">us-east-1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">c</span><span class="p">.</span><span class="nf">sagemaker_inference_deploy_pipeline</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">ecr_image_uri</span><span class="p">,</span> <span class="n">execution_role_arn</span><span class="p">,</span> <span class="n">env_vars</span><span class="p">,</span> <span class="n">instance_type</span><span class="p">)</span>    
</code></pre></div></div> <p>Easy, right? In the example above, I’m specifying that the endpoint should be launched in a GPU-enabled machine. For CPU, you need to change the instance type and the <code class="language-plaintext highlighter-rouge">DEVICE</code> environment variable.</p> <h4 id="inference-speed-profiling-local-vs-aws-sagemaker">Inference speed profiling (local vs. AWS SageMaker)</h4> <p>Inference times are important. There are too many GPU types, configurations, and model sizes to take into account, so I created some scripts to test the model’s performance. We tried three model sizes (<code class="language-plaintext highlighter-rouge">s</code>, <code class="language-plaintext highlighter-rouge">l</code> and <code class="language-plaintext highlighter-rouge">x</code>) running locally or on AWS, either GPU or CPU. To compute inference times and standard deviations, 100 requests were made to the API using random COCO val2017 images. Only the inference step was measured to exclude any other I/O latencies. The local machine has a GeForce RTX 2070 and a CPU i7-8700K and the AWS machines used were ml.m5.large for CPU and ml.g4dn.xlarge for GPU.</p> <p>Here’s the results:</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dfine_speed_profiling-480.webp 480w,/assets/img/dfine_speed_profiling-800.webp 800w,/assets/img/dfine_speed_profiling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dfine_speed_profiling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Notice the change in scale between the two graphs. </div> <p>Looking at the CPU results, I would say that only the largest model (<code class="language-plaintext highlighter-rouge">dfine_x</code>) is not suitable for production in AWS. However, that may change if different CPU models are evaluated. For GPU performance, all methods are, on average, under 100ms for inference which is incredible – D-FINE-X model, which achieves 59.3% AP on COCO, included. Still, these times can be improved using other frameworks, such as TensorRT or ONNX Runtime.</p> <h4 id="thats-it">That’s it</h4> <p>I hope the snippets above help you. Contact me if you have any doubts about reproducing this post. I was surprised by how well D-FINE models behaved both in accuracy and speed, so that might be a game changer for me. I’m currently studying the many papers that stem from the original DETR work and might do a post on that.</p> <h4 id="references">References</h4> <p>[1] - Peng, Yansong, et al. “D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement.” arXiv preprint arXiv:2410.13842 (2024).</p> <p>[2] - Huang, Shihua, et al. “DEIM: DETR with Improved Matching for Fast Convergence.” arXiv preprint arXiv:2412.04234 (2024).</p> <p>[3] - Zhao, Yian, et al. “Detrs beat yolos on real-time object detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.</p> <p>[4] - Carion, Nicolas, et al. “End-to-end object detection with transformers.” European conference on computer vision. Cham: Springer International Publishing, 2020.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/">A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/">My favorite papers from CVPR 2022. | by Gustavo Führ | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/screenshot-query/">ScreenshotQuery, make queries to screenshots using Vision Language Models.</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gustavo Führ. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZR2Y0F7K5S"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZR2Y0F7K5S");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>