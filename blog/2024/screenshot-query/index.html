<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ScreenshotQuery, make queries to screenshots using Vision Language Models. | Gustavo Führ </title> <meta name="author" content="Gustavo Führ"> <meta name="description" content="Describe and talk with your set of screenshots"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?491bdee39dab28e6dfd8bfd8a4b8ac78"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gustavofuhr.github.io/blog/2024/screenshot-query/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Gustavo Führ </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repos </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ScreenshotQuery, make queries to screenshots using Vision Language Models.</h1> <p class="post-meta"> Created in December 04, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   <a href="/blog/tag/vlm"> <i class="fa-solid fa-hashtag fa-sm"></i> VLM</a>   <a href="/blog/tag/image-retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> image-retrieval</a>   <a href="/blog/tag/image-caption"> <i class="fa-solid fa-hashtag fa-sm"></i> image-caption</a>   <a href="/blog/tag/semantic-search"> <i class="fa-solid fa-hashtag fa-sm"></i> semantic-search</a>   ·   <a href="/blog/category/posts"> <i class="fa-solid fa-tag fa-sm"></i> posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/screenshot_query_03.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> A notebook to query images. </div> <p>A couple of months ago I started a small project that I thought would be a simple task from my ever growing to-do list: processing approximately 1000 screenshots so I can perform some kind of image retrieval using natural language on them. It seemed simple enough yet it had some details that I would like to share in this post.</p> <p>Here’s the <a href="https://github.com/gustavofuhr/screenshot_query" rel="external nofollow noopener" target="_blank">project at Github</a>.</p> <p>🚨 Before we start: Screenshots are usually very personal and might contain a bunch of personal information. If you want to use this code, ensure that you are comfortable sending them to OpenAI or other LLMs.</p> <h4 id="which-problem-are-we-tackling-here">Which problem are we tackling here?</h4> <p>It’s important to clarify that we’ll delegate the image captioning to an MLLM model, even if there are other <a href="https://paperswithcode.com/task/image-captioning" rel="external nofollow noopener" target="_blank">available models</a> can run locally. If I started again I would probably try to use <a href="https://paperswithcode.com/paper/blip-2-bootstrapping-language-image-pre" rel="external nofollow noopener" target="_blank">BLIP-2</a> to generate captions/embeddings.</p> <p>So, our ultimate goal is to search images through textual content descriptions using natural language. Image retrieval (and scene classification) has a long history in Computer Vision – remember the seminal <a href="https://people.eecs.berkeley.edu/~efros/courses/AP06/Papers/csurka-eccv-04.pdf" rel="external nofollow noopener" target="_blank">Bag of Visual Words</a> model? <em>Yet, here we’re only trying to use natural language to query a set of image descriptions; everything is text</em>. The most appropriate nomenclature I found was “similarity search”, which is sometimes used in (now trending) RAG systems.</p> <p>Anyway, the next sections discuss the steps that I took to achieve the objective.</p> <h3 id="1-creating-image-descriptions">1. Creating image descriptions</h3> <p>The first thing is to send to a MLLM (Multi-modal Large Language Model), such as GPT4-o, all the images with a simple prompt asking the model to describe the image contents:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">get_image_description_openai</span><span class="p">(</span><span class="n">base64_image</span><span class="p">):</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Content-Type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">application/json</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Authorization</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Bearer </span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">OPENAI_API_KEY</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
    <span class="p">}</span>

    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What’s in this image?</span><span class="sh">"</span>
                <span class="p">},</span>
                <span class="p">{</span>
                <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">image_url</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">image_url</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="sh">"</span><span class="s">url</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">data:image/jpeg;base64,</span><span class="si">{</span><span class="n">base64_image</span><span class="si">}</span><span class="sh">"</span>
                <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">]</span>
            <span class="p">}</span>
        <span class="p">],</span>
        <span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">300</span>
    <span class="p">}</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">"</span><span class="s">https://api.openai.com/v1/chat/completions</span><span class="sh">"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">).</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="sh">"</span><span class="s">choices</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span>

</code></pre></div></div> <p>You can find the whole script <a href="https://github.com/gustavofuhr/screenshot_query/blob/main/generate_image_descriptions_openai.py" rel="external nofollow noopener" target="_blank">here</a>. That may take a lot of time, but it can be resumed multiple times. It’s also surprisingly cheap to send those images to OpenAI (for 1k images I guess it was around 2-3 USD total!). The script will create txt files describing the images, one for each image.</p> <h4 id="how-good-are-the-descriptions">How good are the descriptions?</h4> <p>It’s impressively good! Check these:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideshadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_02-480.webp 480w,/assets/img/screenshot_query/sq_desc_02-800.webp 800w,/assets/img/screenshot_query/sq_desc_02-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sq_desc_02.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_01-480.webp 480w,/assets/img/screenshot_query/sq_desc_01-800.webp 800w,/assets/img/screenshot_query/sq_desc_01-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sq_desc_01.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_03-480.webp 480w,/assets/img/screenshot_query/sq_desc_03-800.webp 800w,/assets/img/screenshot_query/sq_desc_03-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sq_desc_03.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sq_desc_04-480.webp 480w,/assets/img/screenshot_query/sq_desc_04-800.webp 800w,/assets/img/screenshot_query/sq_desc_04-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sq_desc_04.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <h3 id="2-making-queries">2. Making queries</h3> <p>I got two main ideas for this:</p> <ul> <li>Create text embeddings for each description and perform an ANN (Approximate Nearest Neighbors) search for the query.</li> <li>Send the text containing all the image descriptions to the LLM with a prompt to retrieve the top-N most relevant image depictions.</li> </ul> <h4 id="emb">2.1 Text embeddings</h4> <p>The idea here is to represent the whole description of the image contents as a vector. That will also be done later for the query so we can search the closest embeddings to the query, under a certain distance threshold.I chose to use ANN since K-nearest neighbors will be too slow for larger datasets, in case I want to process my entire gallery someday.</p> <p>We’ll explore two ways to create these embeddings, but first, let’s talk about how we can store them and perform search. This is usually done with a vector database, and we can find several solutions for it: <a href="https://milvus.io/" rel="external nofollow noopener" target="_blank">Milvus</a>, <a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview" rel="external nofollow noopener" target="_blank">Vertex AI</a> from Google, <a href="https://github.com/weaviate/weaviate" rel="external nofollow noopener" target="_blank">Weaviate</a> etc. I had some experience in the first two but want to try Weaviate since it appeared to have a very easy setup. For search, it appears that Weaviate uses the <a href="https://arxiv.org/abs/1603.09320" rel="external nofollow noopener" target="_blank">HNSW algorithm</a> for ANN (Approximate Nearest Neighbors) which is probably available in the other solutions too – It’s worth mentioning that the awesome <a href="https://github.com/facebookresearch/faiss" rel="external nofollow noopener" target="_blank">Faiss</a> has implementations of several ANN algorithms if one wants to explore different methods.</p> <p>Using Weaviate was indeed very easy. Here’s how to start its Docker container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-p</span> 8080:8080 <span class="nt">-p</span> 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.26.3
</code></pre></div></div> <p>One thing that I found kind of annoying is that they recently changed the interface of the Python library. The version used in the following snippets is <code class="language-plaintext highlighter-rouge">weaviate-client==4.7.1</code>. First, you need to create a class, which is like a collection or a table if you’re not familiar with the term.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span> <span class="o">=</span> <span class="n">weaviate</span><span class="p">.</span><span class="nc">Client</span><span class="p">(</span><span class="n">WEAVIATE_URL</span><span class="p">)</span>

<span class="n">class_obj</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">class</span><span class="sh">"</span><span class="p">:</span> <span class="n">CLASS_NAME</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dataType</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">]},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dataType</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">]}</span>
    <span class="p">]</span>
<span class="p">}</span>
<span class="n">client</span><span class="p">.</span><span class="n">schema</span><span class="p">.</span><span class="nf">create_class</span><span class="p">(</span><span class="n">class_obj</span><span class="p">)</span>
</code></pre></div></div> <p>After that you can populate the class with the embeddings that you want:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">client</span><span class="p">.</span><span class="n">data_object</span><span class="p">.</span><span class="nf">create</span><span class="p">({</span><span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">image.png</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">This is an image containing...}, 
                                CLASS_NAME, vector=description_embedding_vector)
</span></code></pre></div></div> <p>For searching you just need to provide three parameters: the query embedding, the min distance and the limit of objects to return:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="n">query</span><span class="p">.</span><span class="nf">near_vector</span><span class="p">(</span>
        <span class="n">near_vector</span><span class="o">=</span><span class="n">query_embedding</span><span class="p">,</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">distance</span><span class="p">,</span>
        <span class="n">limit</span><span class="o">=</span><span class="n">n_images_limit</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>For creating the embeddings we explore two alternatives: SBERT and OpenAI API.</p> <h4 id="211-sbert-embeddings">2.1.1 SBERT embeddings</h4> <p>A couple of years ago, <a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank">BERT</a> was the main option for creating text embeddings. It was used in a bunch of downstream tasks quite successfully. However, for comparing sentences, which is our goal, it’s not well suited mainly because it doesn’t generate fixed-size embeddings. If you want to use BERT for our case (semantic search), it would take a lot of time. From the <a href="https://arxiv.org/pdf/1908.10084" rel="external nofollow noopener" target="_blank">SBERT paper</a> abstract we have an impressive example:</p> <blockquote> <p>Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT.</p> </blockquote> <p>SBERT fixes this by focusing on semantic similarity tasks. With it, it’s possible to generate fixed embeddings for sentences (or paragraphs). For the above example, SBERT can reduce the elapsed time from ~65 hours to 5 seconds with the same accuracy.</p> <p>Here’s the code: (<a href="https://huggingface.co/sentence-transformers" rel="external nofollow noopener" target="_blank">Reference</a>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">feat</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div> <p>This will generate embeddings with a size of 384.</p> <h4 id="212-openai-embeddings">2.1.2 OpenAI embeddings</h4> <p>It’s expected that modern LLMs provide a way to get embeddings from text, and that is indeed the case. Since I wanted to use only the $10 USD credits I added to OpenAI for this project, I’m going to use their “text-embedding-3-small” model. Generating embeddings on the OpenAI API is super simple:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">text-embedding-3-small</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">text</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">"</span><span class="s">https://api.openai.com/v1/embeddings</span><span class="sh">"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">).</span><span class="nf">json</span><span class="p">()</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div> <p>Notice that there are <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models#embedding-models" rel="external nofollow noopener" target="_blank">larger models available</a> for computing embeddings that should give us better results. I just finished a small project on Text-to-SQL, and in that case, the use of larger models was indeed the deciding factor between success and failure (GPT4-o instead of GPT4-o-mini).</p> <h3 id="22-results-using-embeddings">2.2 Results using embeddings</h3> <p>Once the embeddings are created and stored in the Weaviate collection, we can query them, as we explained in the <a href="#emb">previous section</a>. To see if it is working I’m showing results for 4 different queries:</p> <ul> <li>Image showing any type of scale models.</li> <li>Screenshots showing smartphone lock screens.</li> <li>Screenshots that show research papers.</li> <li>Which screenshots show 3D printing models/objects or 3D printers.</li> </ul> <p>Here’s some results:</p> <p>SBERT embeddings:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideshadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_scale-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_scale-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_scale-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sbert/cropped_scale.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_screen-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_screen-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_screen-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sbert/cropped_screen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_research-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_research-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_research-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sbert/cropped_research.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/sbert/cropped_3D-480.webp 480w,/assets/img/screenshot_query/sbert/cropped_3D-800.webp 800w,/assets/img/screenshot_query/sbert/cropped_3D-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/sbert/cropped_3D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <p>It works, but there are some images with correct descriptions that should not be associated with the query. For instance, the first query has nothing to do with the screenshot of a research paper (4th screenshot in the first image), even though the description of the image is correct:</p> <blockquote> <p>The image seems to be a figure from a research paper titled “LOLNeRF: Learn from One Look.” It illustrates a method for reconstructing 3D models from 2D images using a neural network. Key components include:</p> <ul> <li> <strong>Authors</strong>: The names of researchers associated with different institutions are listed.</li> <li> <strong>Method Overview</strong>: It outlines a process involving coarse pose estimation and the use of a conditioned NeRF model for training and testing.</li> <li> <strong>Images</strong>: Multiple images of faces appear to depict the results of the model, showing various views or representations of different individuals. The figure is likely meant to summarize the approach and findings of the research visually.</li> </ul> </blockquote> <p>The query about research papers also did not return very good examples. It has some weird matches that are not articles/papers at all.</p> <p>Open AI embeddings:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true" slideshadows="false"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_scale-480.webp 480w,/assets/img/screenshot_query/openai/cropped_scale-800.webp 800w,/assets/img/screenshot_query/openai/cropped_scale-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/openai/cropped_scale.png" class="img-fluid rounded" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_screen-480.webp 480w,/assets/img/screenshot_query/openai/cropped_screen-800.webp 800w,/assets/img/screenshot_query/openai/cropped_screen-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/openai/cropped_screen.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_research-480.webp 480w,/assets/img/screenshot_query/openai/cropped_research-800.webp 800w,/assets/img/screenshot_query/openai/cropped_research-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/openai/cropped_research.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/screenshot_query/openai/cropped_3D-480.webp 480w,/assets/img/screenshot_query/openai/cropped_3D-800.webp 800w,/assets/img/screenshot_query/openai/cropped_3D-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/screenshot_query/openai/cropped_3D.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <p>The results are similar but with fewer errors. That said, this entire analysis was qualitative. Conducting more structured experiments would be far more interesting (well, next time then).</p> <h3 id="23-asking-chatgpt-to-search">2.3 Asking chatGPT to search</h3> <p>LLMs perform incredibly well on zero-shot tasks, but sometimes it’s hard to have an intuition if they will solve your problem. I tried to send descriptions of all images with an instruction prompt like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_context</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Your job is to find the top {N_TOP_IMAGES} best screenshots based on a query and the image descriptions. 

Bellow there a bunch of image descriptors preceded by their filename. Make sure that, in your answer
you only include the image filenames in order from most relevant to least relevant from the chosen {N_TOP_IMAGES}.

Example, given a query like this:

</span><span class="sh">"</span><span class="s">Which screenshots appear to be tech products?</span><span class="sh">""</span><span class="s">

You should answer like, without text before this and without numbers before the image names (THIS IS AN FORMAT EXAMPLE, THESE IMAGES DON</span><span class="sh">'</span><span class="s">T EXISTS ):
IMAGE_01.PNG
IMAGE_02.jpeg
</span><span class="gp">...</span>

<span class="s">First there is a list of image filenames, only answer with these filenames in the order of relevance:

{image_filenames}

The following is a list of image descriptor (one for each image), you should use these to answer the query.

{image_descriptors}

</span><span class="sh">"""</span>
</code></pre></div></div> <p>Only using prompt engineering like that may work, but has two annoying problems. First, it is very easy to reach a token limit, even for less than a thousand images (one may be tempted to use Gemini or other APIs that offer longer context windows, but that won’t scale). Second, it is hard to force coherent outputs. For smaller models, it even returns image names that do not exist, and sometimes, the results come with a text preamble that changes each time and that needs to be removed for processing. In my research in text2SQL, that was mitigated using larger models (which don’t hallucinate as much) and by searching the part of the answer that actually corresponds to a formal SQL language.</p> <h2 id="3-thats-it">3. That’s it.</h2> <p>Using LLMs to help with side projects is awesome, especially if you are lazy. Things that would require two or three days of coding can be done in a few hours – with less quality for sure, but still a functional prototype that will give you an insight into whether your idea makes sense or not. That’s where this project comes from, the whole “chatGPT should work for that” new attitude that is changing programming for good. It’s still not as good as 90% of people that hype it up say, but it’s fun.</p> <p>I’m especially keen to ask the LLMs to structure the solution for the problem itself, so you can discover new ideas and tools that would be a pain to research by yourself.Asking how to solve the problem described here, chatGPT suggested using SBERT and Weaviate, which were outside my radar before that. Still, I think that the ideas here need a little more theoretical foundation so that we understand why sometimes completely unrelated texts may present near embeddings; maybe I will revisit this in the future.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/a-study-on-beer-logo-detection-and-analysis-on-social-media-by-meerkat-cv-medium/">A study on beer: logo detection and analysis on social media | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/pushing-the-boundaries-of-face-recognition-systems-by-meerkat-cv-medium/">Pushing the boundaries of Face Recognition systems | by Meerkat Cv | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/my-favorite-papers-from-cvpr-2022-by-gustavo-fhr-medium/">My favorite papers from CVPR 2022. | by Gustavo Führ | Medium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deploy-dfine-models/">Deploying state-of-the-art object detectors (DETRs) to AWS.</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Gustavo Führ. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZR2Y0F7K5S"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZR2Y0F7K5S");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>